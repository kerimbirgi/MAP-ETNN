# @package _global_
model:
  num_hidden: 128

eval_only: true

training:
  epoch: 200
  batch_size: 32
  weight_decay: 1e-16
  scheduler_mode: 'cosine_annealing'
  optimizer: 'Adam'
  min_lr: 1e-5
  num_lr_cycles: 2
  clip_gradients: true
  clip_amount: 1.0
  lr: 5e-4
  crit: 'L1Loss'
  continue_from_weights: false
  normalize_targets: false
  early_stop: true
  early_stop_patience: 20